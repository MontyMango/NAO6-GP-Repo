# Docker-compose help comes from: ai/chat-demo:latest

services:
  # nodejs:
  #   build: .
  #   container_name: nodejs
  #   ports:
  #     - "8080:8080"

  api-server:
    container_name: api-server
    build: "./Backend (API Server)/"
    restart: always
    ports:
      - "45679:45679"
    networks:
      backend:
        ipv4_address: 10.0.60.3
  # This makes the ollama container wait infinitely
  #   depends_on:
  #     ollama:
  #       condition: service_healthy


  ollama:
    container_name: ollama
    image: ollama/ollama
    volumes:
     - '/ollama:/ollama'  # You might need to remove the ./ and replace it with you home directory (Docker doesn't like this)
    # Port to expose on the machine : port to expose in the container
    ports:
     - 11434:11434
    # Check to see if the container is running 
    healthcheck:
      test: ["CMD-SHELL", "curl -s http://10.0.60.4:11434/api/tags | jq -e \".models[] | select(.name == \\\"${MODEL:-mistral:latest}\\\")\" > /dev/null"]
      interval: 10s
      timeout: 5s
      retries: 50
      start_period: 600s
    # If we need to limit the AI model, we can use this
    deploy:
      resources:
        limits:
          cpus: '4'   # Preferrably 4 for lightweight models, 16 for heavy models
          memory: 8G
        reservations:
          cpus: '0.25'
          memory: 2G  # Lightweight models require 2GB's of memory
    restart: always
    networks:
      backend:
        ipv4_address: 10.0.60.2

volumes:
  ollama_data:
    name: ollama_data

networks:
  backend:
    driver: bridge
    ipam:
      config:
        - subnet: "10.0.60.0/28"
