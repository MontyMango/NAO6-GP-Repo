# Docker-compose help comes from: ai/chat-demo:latest

services:
  api-server:
    container_name: speech-recognition-server
    build: "./speech-recognition-api/"
    restart: always
    ports:
      - "45689:45689"
    networks:
      backend:
        ipv4_address: 10.0.60.3

  # api-server:
  #   container_name: api-server
  #   build: "./Backend (API Server)/"
  #   restart: always
  #   ports:
  #     - "45679:45679"
  #   networks:
  #     backend:
  #       ipv4_address: 10.0.60.3
  # This makes the ollama container wait infinitely
  #   depends_on:
  #     ollama:
  #       condition: service_healthy


  ollama:
    container_name: ollama
    image: ollama/ollama
    volumes:
     - '/ollama:/ollama'  # You might need to remove the ./ and replace it with you home directory (Docker doesn't like this)
    # Port to expose on the machine : port to expose in the container
    ports:
     - 11434:11434
    # If we need to limit the AI model, we can use this
    deploy:
      resources:
        limits:
          cpus: '4'   # Preferrably 4 for lightweight models, 16 for heavy models
          memory: 8G
        reservations:
          cpus: '0.25'
          memory: 2G  # Lightweight models require 2GB's of memory
    restart: always
    networks:
      backend:
        ipv4_address: 10.0.60.2

volumes:
  ollama_data:
    name: ollama_data

networks:
  backend:
    driver: bridge
    ipam:
      config:
        - subnet: "10.0.60.0/28"
