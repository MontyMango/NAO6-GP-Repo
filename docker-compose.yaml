# Docker-compose help comes from: ai/chat-demo:latest

services:
  # nodejs:
  #   build: .
  #   container_name: nodejs
  #   ports:
  #     - "8080:8080"

  backend:
    container_name: api-server
    build: "./Backend (API Server)/"
    ports:
      - "45679:45679"
    environment:
      - MODEL_HOST=http://ollama:11434
    depends_on:
      ollama:
        condition: service_healthy


  ollama:
    container_name: ollama
    image: ollama/ollama
    volumes:
     - '/ollama:./.ollama'  # You might need to remove the ./ and replace it with you home directory (Docker doesn't like this)
    # Port to expose on the machine : port to expose in the container
    ports:
     - 11434:11434
    # Check to see if the container is running 
    healthcheck:
      test: ["CMD-SHELL", "curl -s http://localhost:11434/api/tags | jq -e \".models[] | select(.name == \\\"${MODEL:-mistral:latest}\\\")\" > /dev/null"]
      interval: 10s
      timeout: 5s
      retries: 50
      start_period: 600s
    # If we need to limit the AI model, we can use this
    deploy:
      resources:
        limits:
          cpus: '4'   # Preferrably 4 for lightweight models, 16 for heavy models
          memory: 8G
        reservations:
          cpus: '0.25'
          memory: 2G  # Lightweight models require 2GB's of memory
    restart: on-failure

volumes:
  ollama_data:
    name: ollama_data

networks:
  backend:
    ipam:
      driver: default
      config:
        - subnet: "172.16.238.0/31"
